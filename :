from __future__ import division
# library
# standard library
import os

# third-party library
import torch
import torch.nn as nn
import torch.utils.data as Data
import torchvision
# import matplotlib.pyplot as plt
'''Train CIFAR10 with PyTorch.'''
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torch.backends.cudnn as cudnn

import torchvision
import torchvision.transforms as transforms

import os
import argparse

from models import *
from utils import progress_bar

import networkx as nx
import matplotlib.pyplot as plt
import pandas as pd
import random
import copy

parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')
parser.add_argument('--lr', default=0.1, type=float, help='learning rate')
parser.add_argument('--resume', '-r', action='store_true',
                    help='resume from checkpoint')
args = parser.parse_args()

# device = 'cuda' if torch.cuda.is_available() else 'cpu'
device = 'cpu'
best_acc = 0  # best test accuracy
start_epoch = 0  # start from epoch 0 or last checkpoint epoch

# Data
print('==> Preparing data..')
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

trainset = torchvision.datasets.CIFAR10(
    root='/home/fahao/CIFAR/GC-cifar/cifar-10-batches-py/', train=True, download=True, transform=transform_train)
trainloader = torch.utils.data.DataLoader(
    trainset, batch_size=128, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(
    root='/home/fahao/CIFAR/GC-cifar/cifar-10-batches-py/', train=False, download=True, transform=transform_test)
testloader = torch.utils.data.DataLoader(
    testset, batch_size=100, shuffle=False, num_workers=2)

classes = ('plane', 'car', 'bird', 'cat', 'deer',
           'dog', 'frog', 'horse', 'ship', 'truck')


# Parallel Parameters
CLIENT = 30 # number of clients
Model = [None for i in range (CLIENT)]
Optimizer = [None for i in range (CLIENT)]

# small world
G = nx.watts_strogatz_graph(n = 30, k = 2, p = 0.5)
latency = [[0 for i in range (CLIENT)]for j in range (CLIENT)]
for i in range (CLIENT):
    for j in range (CLIENT):
        latency[i][j] = random.randint(1,20)


# # Model
print('==> Building model..')
    # net = VGG('VGG19')
    # net = ResNet18()
    # net = PreActResNet18()
    # net = GoogLeNet()
    # net = DenseNet121()
    # net = ResNeXt29_2x64d()
    # net = MobileNet()
    # net = MobileNetV2()
    # net = DPN92()
    # net = ShuffleNetG2()
    # net = SENet18()
    # net = ShuffleNetV2(1)
    # net = EfficientNetB0()
for i in range (CLIENT):
    Model[i] = MobileNet()
    Model[i] = Model[i].to(device)
    if device == 'cuda':
        Model[i] = torch.nn.DataParallel(Model[i])
        cudnn.benchmark = True
    Optimizer[i] = optim.SGD(Model[i].parameters(), lr=args.lr,
                    momentum=0.9, weight_decay=5e-4)
    
global_model = MobileNet()

criterion = nn.CrossEntropyLoss()
# optimizer = optim.SGD(net.parameters(), lr=args.lr,
#                       momentum=0.9, weight_decay=5e-4)


# Training
def train(epoch,Model,Optimizer):
    print('\nEpoch: %d' % epoch)
    train_loss = [0 for i in range (CLIENT)]
    correct = [0 for i in range (CLIENT)]
    total = [0 for i in range (CLIENT)]
    Loss = [0 for i in range (CLIENT)]
    P = [None for i in range (CLIENT)]
    for batch_idx, (inputs, targets) in enumerate(trainloader):
        if batch_idx < 120:
            client = batch_idx % 30
            Model[client].train()
            inputs, targets = inputs.to(device), targets.to(device)
            Optimizer[client].zero_grad()
            outputs = Model[client](inputs)
            Loss[client] = criterion(outputs, targets)
            Loss[client].backward()
            Optimizer[client].step()

            train_loss[client] += Loss[client].item()
            _, predicted = outputs.max(1)
            total[client] += targets.size(0)
            correct[client] += predicted.eq(targets).sum().item()

            progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'
                        % (train_loss[client]/(batch_idx+1), 100.*correct[client]/total[client], correct[client], total[client]))
    for i in range (CLIENT):
        P[i] = Model[i].state_dict()
    return P

def Test(epoch,net):
    global best_acc
    net.eval()
    test_loss = 0
    correct = 0
    total = 0
    acc = 0 
    with torch.no_grad():
        for batch_idx, (inputs, targets) in enumerate(testloader):
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = net(inputs)
            loss = criterion(outputs, targets)

            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'
                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))
            if acc < (round(correct/total,4)*100):
                acc = (round(correct/total,4)*100)
    # Save checkpoint.
    if acc > best_acc:
        print('Saving..')
        state = {
            'net': net.state_dict(),
            'acc': acc,
            'epoch': epoch,
        }
        if not os.path.isdir('checkpoint'):
            os.mkdir('checkpoint')
        torch.save(state, './checkpoint/ckpt.pth')
        best_acc = acc
    return acc
# aggregate local model
def aggre_local(model, i, Model):
    time = 0
    Q = []
    P = copy.deepcopy(model.state_dict())
    for j in range (CLIENT):
        Q.append(copy.deepcopy(Model[j].state_dict()))
    for key, value in P.items():
        m = 0
        for j in range (CLIENT):
            if G.has_edge(i,j): 
                P[key] = random.random()*P[key] + random.random()*Q[j][key]
                #P[key] = torch.true_divide(P[key],2)
                m = m + 1
        # P[key] = torch.true_divide(P[key],m+1)
        
    for j in range (CLIENT):
        if G.has_edge(i,j):
            time += latency[i][j]
    return P, time
# aggregate global model
def aggre_global(Model):
    temp = []
    for x in range (CLIENT):
        temp.append(copy.deepcopy(Model[x].state_dict()))
    for key, value in temp[0].items():  
        for i in range (CLIENT):
            if i != 0:
                temp[0][key] += temp[i][key]
        temp[0][key] = torch.true_divide(temp[0][key],CLIENT)
    return temp[0]   
# Step time
def step_time(T):
    temp = 0
    for i in range (CLIENT):
        if temp<T[i]:
            temp = T[i]
    return temp


if __name__ == '__main__':
    for i in range (100):
        Csv = 'CIFAR-GC-' + str(i) + '.csv'
        score = []
        times = []
        t = 0    
        P = []

        for epoch in range(start_epoch, start_epoch+50):
            # Load checkpoint.
            if args.resume:
                print('==> Resuming from checkpoint..')
                assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'
                checkpoint = torch.load('./checkpoint/ckpt.pth')
                for i in range (CLIENT):
                    Model[i].load_state_dict(checkpoint['net'])
                best_acc = checkpoint['acc']
                start_epoch = checkpoint['epoch']
            Tim = [0 for i in range (CLIENT)]
            Loss = [0 for i in range (CLIENT)]
            P = train(epoch,Model,Optimizer)
            for i in range (CLIENT):
                Model[i].load_state_dict(P[i])

            

            # for batchs, (inputs, targets) in enumerate(trainloader):
            #     inputs, targets = inputs.to(device), targets.to(device)
            #     if(batchs < 50):
                    
            #         client = batchs % 10
            #         print('Client: ', client, 'Step: ',round(batchs/10))
            #         #client = 0
            #         Optimizer[client].zero_grad()
            #         outputs = Model[client](inputs)
            #         loss = criterion(outputs, targets)
            #         loss.backward()
            #         Optimizer[client].step()
            #     else:
            #         break

            #PP = aggre_global(Model)

            # global model   
            Q = aggre_global(Model) 
            # for client in range (CLIENT):
            #     Model[client].load_state_dict(Q)
            global_model.load_state_dict(Q)
            # for batch_idx, (inputs, targets) in enumerate(testloader):
            #     test_output = global_model(inputs)
            #     pred_y = torch.max(test_output, 1)[1].data.numpy()
            #     accuracy = float((pred_y == targets.data.numpy()).astype(int).sum()) / float(targets.size(0))
            accuracy = Test(epoch,global_model)
            print('Epoch: ', epoch, '| test accuracy: %.2f' % accuracy)
            score.append(accuracy)

            # aggregate local model
            for i in range (3):
                P_new = [None for m in range (CLIENT)]
                for x in range (CLIENT):
                    P_new[x], temp = aggre_local(Model[x],x,Model)
                    Tim[x] += temp
                    # P_new[x],Tim = aggre_local(Model[x],x,Model)
                    
            for client in range (CLIENT):
                Model[client].load_state_dict(P_new[client])

            time = step_time(Tim)
            t += time
            times.append(t)

            # for client in range (CLIENT):
            #     Model[client].load_state_dict(Q)
        dataframe = pd.DataFrame(times, columns=['X'])
        dataframe = pd.concat([dataframe, pd.DataFrame(score,columns=['Y'])],axis=1)
        dataframe.to_csv(Csv,header = False,index=False,sep=',')
        pos = nx.circular_layout(G)  
        plt.figure(figsize = (12, 12)) 
        nx.draw_networkx(G, pos) 
        plt.show()
